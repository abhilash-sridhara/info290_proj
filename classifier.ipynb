{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import sklearn.metrics as smt\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.models import Word2Vec as w2v\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "all_feats={}\n",
    "auth_dict={}\n",
    "auth_map={}\n",
    "auth_models={}\n",
    "\n",
    "l_c=1\n",
    "path='./all_c'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtext(filepath):\n",
    "    text=''    \n",
    "    with open(filepath,'rb') as f:\n",
    "        text=f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(text):\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return 'a'\n",
    "    punkts=[',','.',';','!',':','?',\"''\",\"'\"]\n",
    "    lm = WordNetLemmatizer()\n",
    "    features={}\n",
    "    voc_count = Counter()\n",
    "    text = text.lower()\n",
    "    text = re.sub('\"[\\S\\s]*?\"','',text)\n",
    "    lines = [len(x.split()) for x in sent_tokenize(text)]\n",
    "    avg_word_pl = sum(lines)/len(lines)    \n",
    "    l = [len(x.split()) for x in text.split('\\n')]\n",
    "    avg_word_pp = sum(l)/len(l)\n",
    "    features['avg_word_pp']=avg_word_pp\n",
    "    features['avg_word_pl']=avg_word_pl\n",
    "    toks = word_tokenize(text)\n",
    "    tok_tags = pos_tag(toks)\n",
    "#     print(toks)\n",
    "    lemmas=[]\n",
    "    words=[]\n",
    "    sen=[x.split() for x in sent_tokenize(text)]\n",
    "    model = w2v(sen,workers=4,sg=1,window=2,iter=30,size=25,alpha=0.5,min_count=1)\n",
    "    min_word_depths=Counter()\n",
    "    max_word_depths=Counter()\n",
    "    func_words = []\n",
    "    for word,tag in tok_tags:\n",
    "        if word not in punkts:\n",
    "            words.append(word)\n",
    "            lemmas.append(lm.lemmatize(word,get_wordnet_pos(tag)))\n",
    "            wd_sysnset = wordnet.synsets(word,pos=get_wordnet_pos(tag))\n",
    "            if len(wd_sysnset)>0:\n",
    "                min_word_depths[wd_sysnset[0].min_depth()]+=1\n",
    "                max_word_depths[wd_sysnset[0].max_depth()]+=1\n",
    "            if(get_wordnet_pos(tag) =='a' or get_wordnet_pos(tag) == wordnet.ADV):\n",
    "                func_words.append(word)\n",
    "                voc_count[word] +=1\n",
    "    unk_toks = set(lemmas)\n",
    "    unk_funk = set(func_words)\n",
    "    \n",
    "    f_word_count=[(k,v) for (k,v) in voc_count.items() if k in model.wv.vocab.keys()]\n",
    "    f_word_count.sort(key=lambda x: x[1],reverse=True)\n",
    "    av_distance=0\n",
    "    most_freq=[k for k,_ in f_word_count[:50]]\n",
    "    lest_freq=[k for k,_ in f_word_count[100:]]\n",
    "    for w in most_freq:\n",
    "        av_distance+=sum(model.wv.distances(w,model.wv.vocab.keys()))/len(model.wv.vocab.keys())\n",
    "    av_distance/=len(text.split())/len(func_words)\n",
    "    features['av_dist']=av_distance\n",
    "#     print('av_dist',av_distance)\n",
    "    \n",
    "    \n",
    "    type_tok_ratio = len(unk_toks)/len(toks)\n",
    "    funk_freq = len(unk_funk)/len(toks)\n",
    "    features['type_tok_ratio'] = type_tok_ratio\n",
    "#     features['type_tok_rat'] = len(set(words))/len(words)\n",
    "    features['funk_freq'] = funk_freq\n",
    "    for funk_word in voc_count:\n",
    "        features[funk_word+'_word_count'] = voc_count[funk_word]\n",
    "    for min_depth in min_word_depths:\n",
    "        features['min_word_depth'+str(min_depth)]=min_word_depths[min_depth]\n",
    "    for max_depth in max_word_depths:\n",
    "        features['min_word_depth'+str(max_depth)]=max_word_depths[max_depth]    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_av(text):\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return 'a'\n",
    "    punkts=[',','.',';','!',':','?',\"''\",\"'\"]\n",
    "    voc_count = Counter()\n",
    "    text = text.lower()\n",
    "    text = re.sub('\"[\\S\\s]*?\"','',text)\n",
    "    toks = word_tokenize(text)\n",
    "    tok_tags = pos_tag(toks)\n",
    "    sen=[x.split() for x in sent_tokenize(text)]\n",
    "    model = w2v(sen,workers=4,sg=0,window=4,iter=50,size=25,alpha=0.75,min_count=1)\n",
    "    func_words = []\n",
    "    for word,tag in tok_tags:\n",
    "        if word not in punkts:\n",
    "            if(get_wordnet_pos(tag) =='a' or get_wordnet_pos(tag) == wordnet.ADV):\n",
    "                func_words.append(word)\n",
    "                voc_count[word] +=1\n",
    "    \n",
    "    f_word_count=[(k,v) for (k,v) in voc_count.items() if k in model.wv.vocab.keys()]\n",
    "    f_word_count.sort(key=lambda x: x[1],reverse=True)\n",
    "    av_distance=0\n",
    "    most_freq=[k for k,_ in f_word_count[:100]]\n",
    "    lest_freq=[k for k,_ in f_word_count[100:]]\n",
    "    for w in most_freq:\n",
    "        av_distance+=sum(model.wv.distances(w,most_freq))/len(most_freq)\n",
    "    av_distance/=len(text.split())/len(func_words)\n",
    "    return({'av_dist':av_distance})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auth_model_av_dist():\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return 'a'\n",
    "    punkts=[',',';','!','.',':','?',\"'\",\"''\"]\n",
    "    for entry in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, entry)):\n",
    "            text = readtext(path+\"/\" +entry+'/'+entry+'.txt')\n",
    "            text = text.decode(\"utf-8\", \"replace\")\n",
    "            text=text.lower()\n",
    "            sen=[x.split() for x in sent_tokenize(text)]\n",
    "            model = w2v(sen,workers=4,sg=0,window=5,iter=30,size=25,alpha=0.95,min_count=1)\n",
    "            f_word_vecs=Counter()\n",
    "            toks = word_tokenize(text)\n",
    "            tok_tags = pos_tag(toks)\n",
    "#             print(tok_tags)\n",
    "            for word,tag in tok_tags:\n",
    "                if word not in punkts:\n",
    "                    if(get_wordnet_pos(tag)=='a' or get_wordnet_pos(tag)==wordnet.ADV):\n",
    "#                         print(word)\n",
    "                        f_word_vecs[word]+=1\n",
    "            \n",
    "            f_word_count=[(k,v) for (k,v) in f_word_vecs.items() if k in model.wv.vocab.keys()]\n",
    "            f_word_count.sort(key=lambda x: x[1],reverse=True)\n",
    "            av_distance=0\n",
    "            most_freq=[k for k,_ in f_word_count[:100]]\n",
    "            lest_freq=[k for k,_ in f_word_count[100:]]\n",
    "#             print(most_freq)\n",
    "            for w in most_freq:\n",
    "#                 print(model.wv.distances(w,most_freq))\n",
    "                av_distance+=sum(model.wv.distances(w,most_freq))/len(most_freq)\n",
    "            av_distance/=len(text.split())/len(f_word_vecs)\n",
    "#             print('av_distance',av_distance,entry)\n",
    "            auth_models[entry]=av_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#get_auth_model_av_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    get_auth_model_av_dist() # load author av_dist \n",
    "    data=[]\n",
    "    c=0\n",
    "#     print(path)\n",
    "    for entry in os.listdir(path):\n",
    "        av_dist=[]\n",
    "        if os.path.isdir(os.path.join(path, entry)):\n",
    "            text = readtext(path+\"/\" +entry+'/'+entry+'.txt')\n",
    "            text = text.decode(\"utf-8\", \"replace\")\n",
    "            for article in text.split('\\t'):\n",
    "                if(len(article)>0):\n",
    "                    feats = featurize(article)\n",
    "#                     feats.update({'av_dist':auth_models[entry]})\n",
    "                    data.append((entry,feats))\n",
    "                    av_dist += [feats['av_dist']]\n",
    "            print('auth',entry,'av',sum(av_dist)/len(av_dist))\n",
    "            print(av_dist)\n",
    "            print()\n",
    "            auth_dict[c]=entry\n",
    "            auth_map[entry]=c\n",
    "            c+=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    data=[]\n",
    "    for entry in os.listdir(path):\n",
    "#         print(entry)\n",
    "#         print(os.path.join(path, entry+'/test'))\n",
    "        if os.path.isdir(os.path.join(path, entry+'/test')):\n",
    "#             print(Path(path+'\\\\'+entry+'\\\\test'))\n",
    "            for tst_file in os.listdir(path+'/'+entry+'/test'):\n",
    "                if(tst_file.endswith('.txt')):\n",
    "                    text = readtext(path+'/'+entry+'/test/'+tst_file)\n",
    "                    text = text.decode(\"utf-8\", \"replace\")\n",
    "                    for article in text.split('\\t'):\n",
    "                        if(len(article)>0):\n",
    "                            feats = featurize(article)\n",
    "#                             feats.update(find_av(article))\n",
    "                            data.append((entry,feats))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(get_train_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_test_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(l2_p):\n",
    "    global all_feats\n",
    "    fid=0\n",
    "    data = get_train_data()\n",
    "    for (label,feats) in data:\n",
    "        for feat in feats:\n",
    "            if feat not in all_feats:\n",
    "                all_feats[feat] = fid\n",
    "                fid+=1\n",
    "    D = len(data)\n",
    "    F = len(all_feats)\n",
    "    X = sparse.dok_matrix((D,F))\n",
    "    Y = np.zeros(len(data))\n",
    "    for idx,(label,feats) in enumerate(data):\n",
    "        for feat in feats:\n",
    "            X[idx,all_feats[feat]] = feats[feat]\n",
    "            Y[idx] = auth_map[label]\n",
    "    logreg = linear_model.LogisticRegression(C=l2_p)\n",
    "    logreg.fit(X,Y)\n",
    "#     print(logreg.score(X,Y))\n",
    "    return(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lg=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(l2_p):\n",
    "    test_data = get_test_data()\n",
    "    lm = create_model(l2_p)\n",
    "    D = len(test_data)\n",
    "    F = len(all_feats)\n",
    "    X = sparse.dok_matrix((D,F))\n",
    "    Y = np.arange(len(test_data),dtype=int)\n",
    "    true_labels=[]\n",
    "    for idx,(label,feats) in enumerate(test_data):\n",
    "        true_labels.append(auth_map[label])\n",
    "        for feat in feats:\n",
    "            if feat in all_feats:\n",
    "                X[idx,all_feats[feat]] = feats[feat]\n",
    "    \n",
    "    preds=lm.predict(X)\n",
    "    probs = lm.predict_proba(X)\n",
    "    idx=0\n",
    "    \n",
    "    p,r,f1, _ = smt.precision_recall_fscore_support(true_labels,preds,warn_for=())\n",
    "    preds = [int(x) for x in preds]\n",
    "    print(preds)\n",
    "    print(true_labels)\n",
    "    print()\n",
    "#     for prob in probs:\n",
    "#         print('true_label',true_labels[idx])\n",
    "#         print(prob)\n",
    "#         idx+=1\n",
    "#     print()\n",
    "    w_c=0\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i]!=true_labels[i]:\n",
    "            w_c+=1\n",
    "    print(w_c,'incorrect out of',len(preds))\n",
    "    print()\n",
    "#     for i in range(len(auth_dict)):\n",
    "#         print('Author',auth_dict[i])\n",
    "#         print('precision',round(p[i],4))\n",
    "#         print('recall',round(r[i],4))\n",
    "#         print('f1score',round(f1[i],4))\n",
    "#         print()\n",
    "#     for i in range(len(true_labels)):\n",
    "#         print('true author:',auth_dict[true_labels[i]])\n",
    "#         print('predicted author:',auth_dict[preds[i]])\n",
    "#         print()\n",
    "    av_f1={'avg f1':round(sum(f1)/len(f1),2)}\n",
    "    print(av_f1)\n",
    "    print('-----------------------')\n",
    "    return(av_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth John Tierney av 19.670161673509323\n",
      "[19.186731402056576, 19.748558134665842, 20.04268476888552, 19.69989631955671, 19.186352039987852, 20.156747375903418]\n",
      "\n",
      "auth Simon Romero av 18.513901642997975\n",
      "[19.125739642324536, 18.917961177077782, 18.30419418615027, 19.008030748205954, 18.19860152295183, 18.434200184636012, 18.44011992880339, 17.37511508845905, 18.821152308372966]\n",
      "\n",
      "auth Adam Liptak av 18.139009238667708\n",
      "[19.57058957125566, 17.047372185160146, 18.54357440733995, 18.651155480780357, 18.225155712955225, 18.66138640886046, 17.49591010642487, 18.270515400755805, 16.7854238744769]\n",
      "\n",
      "auth Alessandra Stanley av 20.550210801147607\n",
      "[19.56492011817019, 19.925933876172554, 20.736279775360522, 21.420354078916006, 20.37753967971675, 21.27623727854962]\n",
      "\n",
      "auth Terry Pristin av 18.452151757008767\n",
      "[17.499867743654033, 19.40384448042395, 17.807047805800828, 19.44203207486707, 18.149475729859912, 18.410642707446826]\n",
      "\n",
      "auth Robert Pear av 17.74031579421867\n",
      "[17.7612365448706, 17.577805655348364, 17.301700738443117, 18.353234008809398, 18.53518053672644, 17.693817432343014, 17.5843928171912, 18.140659974965654, 16.714814439270242]\n",
      "\n",
      "auth Sarah Kreshaw av 17.61408546112294\n",
      "[18.550579099384894, 20.237092960985247, 18.5948736838568, 17.227570849565268, 14.974418474912044, 16.099977698033392]\n",
      "\n",
      "auth Adam Nagoury av 18.156389994715497\n",
      "[18.188944117904597, 18.91351710338613, 17.920787508870053, 19.46220011894013, 17.39272105295621, 17.4765610482253, 18.43565168554789, 18.061894415664373, 17.555232900944784]\n",
      "\n",
      "auth Maureen Dowd av 19.948714197819907\n",
      "[19.809504659486446, 19.255576466005664, 20.545431317014128, 19.701893411245823, 20.03913353587135, 20.18754680037275, 21.380830125171748, 19.039154511738996, 19.579356953472246]\n",
      "\n",
      "auth Nicholas av 20.821879471276326\n",
      "[20.441591297993288, 21.29329095994001, 20.460509224576658, 20.38958900926946, 21.602181585312316, 20.744114750566226]\n",
      "\n",
      "auth Alan Ridding av 19.84280704060583\n",
      "[20.035332127765546, 19.581639208868207, 19.26174228301059, 19.358887558033892, 20.050370592027313, 20.768870473929404]\n",
      "\n",
      "[9, 0, 1, 1, 1, 2, 9, 2, 3, 3, 4, 4, 5, 5, 5, 10, 5, 7, 7, 5, 8, 7, 8, 9, 9, 0, 1]\n",
      "[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10]\n",
      "\n",
      "8 incorrect out of 27\n",
      "\n",
      "{'avg f1': 0.64}\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhilash/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/abhilash/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg f1': 0.64}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_cs = np.arange(0.01,0.03,0.01)\n",
    "# res = []\n",
    "# for lc in l_cs:\n",
    "#     res+=evaluate(lc)\n",
    "# print('best penalty',l_cs[res.index(max(res))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'John Tierney': 0, 'Simon Romero': 1, 'Adam Liptak': 2, 'Alessandra Stanley': 3, 'Terry Pristin': 4, 'Robert Pear': 5, 'Sarah Kreshaw': 6, 'Adam Nagoury': 7, 'Maureen Dowd': 8, 'Nicholas': 9, 'Alan Ridding': 10}\n"
     ]
    }
   ],
   "source": [
    "print(auth_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

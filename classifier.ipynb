{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import sklearn.metrics as smt\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.models import Word2Vec as w2v\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "all_feats={}\n",
    "auth_dict={}\n",
    "auth_map={}\n",
    "auth_models={}\n",
    "\n",
    "l_c=1\n",
    "path='./all_c'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtext(filepath):\n",
    "    text=''    \n",
    "    with open(filepath,'rb') as f:\n",
    "        text=f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(text):\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return 'a'\n",
    "    punkts=[',','.',';','!',':','?',\"''\",\"'\"]\n",
    "    lm = WordNetLemmatizer()\n",
    "    features={}\n",
    "    voc_count = Counter()\n",
    "    text = text.lower()\n",
    "    text = re.sub('\"[\\S\\s]*?\"','',text)\n",
    "    lines = [len(x.split()) for x in sent_tokenize(text)]\n",
    "    avg_word_pl = sum(lines)/len(lines)    \n",
    "    l = [len(x.split()) for x in text.split('\\n')]\n",
    "    avg_word_pp = sum(l)/len(l)\n",
    "    features['avg_word_pp']=avg_word_pp\n",
    "    features['avg_word_pl']=avg_word_pl\n",
    "    toks = word_tokenize(text)\n",
    "    tok_tags = pos_tag(toks)\n",
    "#     print(toks)\n",
    "    lemmas=[]\n",
    "    words=[]\n",
    "    sen=[x.split() for x in sent_tokenize(text)]\n",
    "    model = w2v(sen,workers=4,sg=0,window=4,iter=50,size=25,alpha=0.75,min_count=1)\n",
    "    min_word_depths=Counter()\n",
    "    max_word_depths=Counter()\n",
    "    func_words = []\n",
    "    for word,tag in tok_tags:\n",
    "        if word not in punkts:\n",
    "            words.append(word)\n",
    "            lemmas.append(lm.lemmatize(word,get_wordnet_pos(tag)))\n",
    "            wd_sysnset = wordnet.synsets(word,pos=get_wordnet_pos(tag))\n",
    "            if len(wd_sysnset)>0:\n",
    "                min_word_depths[wd_sysnset[0].min_depth()]+=1\n",
    "                max_word_depths[wd_sysnset[0].max_depth()]+=1\n",
    "            if(get_wordnet_pos(tag) =='a' or get_wordnet_pos(tag) == wordnet.ADV):\n",
    "                func_words.append(word)\n",
    "                voc_count[word] +=1\n",
    "    unk_toks = set(lemmas)\n",
    "    unk_funk = set(func_words)\n",
    "    \n",
    "    f_word_count=[(k,v) for (k,v) in voc_count.items() if k in model.wv.vocab.keys()]\n",
    "    f_word_count.sort(key=lambda x: x[1],reverse=True)\n",
    "    av_distance=0\n",
    "    most_freq=[k for k,_ in f_word_count[:100]]\n",
    "    lest_freq=[k for k,_ in f_word_count[100:]]\n",
    "    for w in most_freq:\n",
    "        av_distance+=sum(model.wv.distances(w,most_freq))/len(most_freq)\n",
    "    av_distance/=len(text.split())/len(func_words)\n",
    "    features['av_distance']=av_distance\n",
    "    \n",
    "    type_tok_ratio = len(unk_toks)/len(toks)\n",
    "    funk_freq = len(unk_funk)/len(toks)\n",
    "    features['type_tok_ratio'] = type_tok_ratio\n",
    "#     features['type_tok_rat'] = len(set(words))/len(words)\n",
    "    features['funk_freq'] = funk_freq\n",
    "    for funk_word in voc_count:\n",
    "        features[funk_word+'_word_count'] = voc_count[funk_word]\n",
    "    for min_depth in min_word_depths:\n",
    "        features['min_word_depth'+str(min_depth)]=min_word_depths[min_depth]\n",
    "    for max_depth in max_word_depths:\n",
    "        features['min_word_depth'+str(max_depth)]=max_word_depths[max_depth]    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_auth_model_vecs():\n",
    "#     def get_wordnet_pos(treebank_tag):\n",
    "#         if treebank_tag.startswith('J'):\n",
    "#             return wordnet.ADJ\n",
    "#         elif treebank_tag.startswith('V'):\n",
    "#             return wordnet.VERB\n",
    "#         elif treebank_tag.startswith('N'):\n",
    "#             return wordnet.NOUN\n",
    "#         elif treebank_tag.startswith('R'):\n",
    "#             return wordnet.ADV\n",
    "#         else:\n",
    "#             return 'a'\n",
    "#     punkts=[',',';','!','.',':','?',\"'\",\"''\"]\n",
    "#     for entry in os.listdir(path):\n",
    "#         if os.path.isdir(os.path.join(path, entry)):\n",
    "#             text = readtext(path+\"/\" +entry+'/'+entry+'.txt')\n",
    "#             text = text.decode(\"utf-8\", \"replace\")\n",
    "#             text=text.lower()\n",
    "#             sen=[x.split() for x in sent_tokenize(text)]\n",
    "#             model = w2v(sen,workers=4,sg=0,window=5,iter=30,size=25,alpha=0.95,min_count=1)\n",
    "#             f_word_vecs=Counter()\n",
    "#             toks = word_tokenize(text)\n",
    "#             tok_tags = pos_tag(toks)\n",
    "# #             print(tok_tags)\n",
    "#             for word,tag in tok_tags:\n",
    "#                 if word not in punkts:\n",
    "#                     if(get_wordnet_pos(tag)=='a' or get_wordnet_pos(tag)==wordnet.ADV):\n",
    "# #                         print(word)\n",
    "#                         f_word_vecs[word]+=1\n",
    "            \n",
    "#             f_word_count=[(k,v) for (k,v) in f_word_vecs.items() if k in model.wv.vocab.keys()]\n",
    "#             f_word_count.sort(key=lambda x: x[1],reverse=True)\n",
    "#             av_distance=0\n",
    "#             most_freq=[k for k,_ in f_word_count[:100]]\n",
    "#             lest_freq=[k for k,_ in f_word_count[100:]]\n",
    "# #             print(most_freq)\n",
    "#             for w in most_freq:\n",
    "# #                 print(model.wv.distances(w,most_freq))\n",
    "#                 av_distance+=sum(model.wv.distances(w,most_freq))/len(most_freq)\n",
    "#             av_distance/=len(text.split())/len(f_word_vecs)\n",
    "#             print('av_distance',av_distance,entry)\n",
    "#             auth_models[entry]=model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#get_auth_model_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_text_model_vecs(text):\n",
    "#     text=text.lower()\n",
    "#     sen=[x.split() for x in sent_tokenize(text)]\n",
    "#     model = w2v(sen,workers=4,sg=1,window=5,iter=100,size=25,alpha=0.5,min_count=2)\n",
    "#     f_word_vecs={}\n",
    "#     toks = word_tokenize(text)\n",
    "#     tok_tags = pos_tag(toks)\n",
    "#     for word,tag in tok_tags():\n",
    "#         if word not in f_word_vecs:\n",
    "#             if(tag=='a' or tag==wordnet.ADV):\n",
    "#                  f_word_vecs[word]=model.wv[word]\n",
    "#     return(f_word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "#     get_auth_model_vecs() # load author word2Vec \n",
    "    data=[]\n",
    "    c=0\n",
    "#     print(path)\n",
    "    for entry in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, entry)):\n",
    "            text = readtext(path+\"/\" +entry+'/'+entry+'.txt')\n",
    "            text = text.decode(\"utf-8\", \"replace\")\n",
    "            for article in text.split('\\t'):\n",
    "                if(len(article)>0):\n",
    "                    feats = featurize(article)\n",
    "                    data.append((entry,feats))\n",
    "            auth_dict[c]=entry\n",
    "            auth_map[entry]=c\n",
    "            c+=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    data=[]\n",
    "    for entry in os.listdir(path):\n",
    "#         print(entry)\n",
    "#         print(os.path.join(path, entry+'/test'))\n",
    "        if os.path.isdir(os.path.join(path, entry+'/test')):\n",
    "#             print(Path(path+'\\\\'+entry+'\\\\test'))\n",
    "            for tst_file in os.listdir(path+'/'+entry+'/test'):\n",
    "                if(tst_file.endswith('.txt')):\n",
    "                    text = readtext(path+'/'+entry+'/test/'+tst_file)\n",
    "                    text = text.decode(\"utf-8\", \"replace\")\n",
    "                    for article in text.split('\\t'):\n",
    "                        if(len(article)>0):\n",
    "                            data.append((entry,featurize(article)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(get_train_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_test_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(l2_p):\n",
    "    global all_feats\n",
    "    fid=0\n",
    "    data = get_train_data()\n",
    "    for (label,feats) in data:\n",
    "        for feat in feats:\n",
    "            if feat not in all_feats:\n",
    "                all_feats[feat] = fid\n",
    "                fid+=1\n",
    "    D = len(data)\n",
    "    F = len(all_feats)\n",
    "    X = sparse.dok_matrix((D,F))\n",
    "    Y = np.zeros(len(data))\n",
    "    for idx,(label,feats) in enumerate(data):\n",
    "        for feat in feats:\n",
    "            X[idx,all_feats[feat]] = feats[feat]\n",
    "            Y[idx] = auth_map[label]\n",
    "    logreg = linear_model.LogisticRegression(C=l2_p)\n",
    "    logreg.fit(X,Y)\n",
    "#     print(logreg.score(X,Y))\n",
    "    return(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lg=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(l2_p):\n",
    "    test_data = get_test_data()\n",
    "    lm = create_model(l2_p)\n",
    "    D = len(test_data)\n",
    "    F = len(all_feats)\n",
    "    X = sparse.dok_matrix((D,F))\n",
    "    Y = np.arange(len(test_data),dtype=int)\n",
    "    true_labels=[]\n",
    "    for idx,(label,feats) in enumerate(test_data):\n",
    "        true_labels.append(auth_map[label])\n",
    "        for feat in feats:\n",
    "            if feat in all_feats:\n",
    "                X[idx,all_feats[feat]] = feats[feat]\n",
    "    \n",
    "    preds=lm.predict(X)\n",
    "    probs = lm.predict_proba(X)\n",
    "    idx=0\n",
    "    \n",
    "    p,r,f1, _ = smt.precision_recall_fscore_support(true_labels,preds,warn_for=())\n",
    "    preds = [int(x) for x in preds]\n",
    "    print(preds)\n",
    "    print(true_labels)\n",
    "    print()\n",
    "#     for prob in probs:\n",
    "#         print('true_label',true_labels[idx])\n",
    "#         print(prob)\n",
    "#         idx+=1\n",
    "#     print()\n",
    "    w_c=0\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i]!=true_labels[i]:\n",
    "            w_c+=1\n",
    "    print(w_c,'incorrect out of',len(preds))\n",
    "    print()\n",
    "#     for i in range(len(auth_dict)):\n",
    "#         print('Author',auth_dict[i])\n",
    "#         print('precision',round(p[i],4))\n",
    "#         print('recall',round(r[i],4))\n",
    "#         print('f1score',round(f1[i],4))\n",
    "#         print()\n",
    "#     for i in range(len(true_labels)):\n",
    "#         print('true author:',auth_dict[true_labels[i]])\n",
    "#         print('predicted author:',auth_dict[preds[i]])\n",
    "#         print()\n",
    "    av_f1={'avg f1':round(sum(f1)/len(f1),2)}\n",
    "    print(av_f1)\n",
    "    print('-----------------------')\n",
    "    return(av_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(l_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhilash/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/abhilash/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 0, 1, 1, 1, 2, 9, 2, 3, 3, 4, 4, 5, 5, 5, 10, 5, 7, 7, 5, 8, 8, 8, 9, 9, 0, 1]\n",
      "[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10]\n",
      "\n",
      "7 incorrect out of 27\n",
      "\n",
      "{'avg f1': 0.67}\n",
      "-----------------------\n",
      "[9, 0, 1, 1, 1, 2, 9, 2, 3, 3, 4, 4, 5, 5, 5, 10, 5, 7, 7, 5, 8, 8, 8, 9, 9, 0, 1]\n",
      "[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10]\n",
      "\n",
      "7 incorrect out of 27\n",
      "\n",
      "{'avg f1': 0.67}\n",
      "-----------------------\n",
      "[9, 0, 1, 1, 1, 2, 9, 2, 3, 3, 4, 4, 5, 5, 5, 10, 5, 7, 7, 5, 8, 8, 8, 9, 9, 0, 1]\n",
      "[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10]\n",
      "\n",
      "7 incorrect out of 27\n",
      "\n",
      "{'avg f1': 0.67}\n",
      "-----------------------\n",
      "[9, 0, 1, 1, 1, 2, 9, 2, 3, 3, 4, 4, 5, 5, 5, 10, 5, 7, 7, 5, 8, 8, 8, 9, 9, 0, 1]\n",
      "[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10]\n",
      "\n",
      "7 incorrect out of 27\n",
      "\n",
      "{'avg f1': 0.67}\n",
      "-----------------------\n",
      "[9, 0, 1, 1, 1, 2, 9, 2, 3, 3, 4, 4, 5, 5, 5, 10, 5, 7, 7, 5, 8, 7, 8, 9, 9, 0, 1]\n",
      "[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10]\n",
      "\n",
      "8 incorrect out of 27\n",
      "\n",
      "{'avg f1': 0.64}\n",
      "-----------------------\n",
      "[9, 0, 1, 1, 1, 2, 9, 2, 3, 3, 4, 4, 5, 5, 5, 10, 5, 7, 7, 5, 8, 7, 8, 9, 9, 0, 1]\n",
      "[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10]\n",
      "\n",
      "8 incorrect out of 27\n",
      "\n",
      "{'avg f1': 0.64}\n",
      "-----------------------\n",
      "[9, 0, 1, 1, 1, 2, 9, 2, 3, 3, 4, 4, 5, 5, 5, 10, 5, 7, 7, 5, 8, 7, 8, 9, 9, 0, 1]\n",
      "[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10]\n",
      "\n",
      "8 incorrect out of 27\n",
      "\n",
      "{'avg f1': 0.64}\n",
      "-----------------------\n",
      "[9, 0, 1, 1, 1, 2, 9, 2, 3, 3, 4, 4, 5, 5, 5, 10, 5, 7, 7, 5, 8, 7, 8, 9, 9, 0, 1]\n",
      "[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10]\n",
      "\n",
      "8 incorrect out of 27\n",
      "\n",
      "{'avg f1': 0.64}\n",
      "-----------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-dfdf7320cb35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml_cs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mres\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best penalty'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml_cs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-87980a4d79d7>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(l2_p)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f865b0ff8e99>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(l2_p)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mall_feats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-5083673616ba>\u001b[0m in \u001b[0;36mget_train_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mauth_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-409737e3771b>\u001b[0m in \u001b[0;36mfeaturize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd_sysnset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mmin_word_depths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwd_sysnset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mmax_word_depths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwd_sysnset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'a'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfunc_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mmax_depth\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mmax_depth\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mmax_depth\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_max_depth\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mhypernyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance_hypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mhypernyms\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_related\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'@'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_hypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l_cs = np.arange(0.01,0.3,0.01)\n",
    "res = []\n",
    "for lc in l_cs:\n",
    "    res+=evaluate(lc)\n",
    "print('best penalty',l_cs[res.index(max(res))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import sklearn.metrics as smt\n",
    "all_feats={}\n",
    "auth_dict={}\n",
    "auth_map={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtext(filepath):\n",
    "    text=''    \n",
    "    with open(filepath,'rb') as f:\n",
    "        text=f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(text):\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return 'a'\n",
    "    \n",
    "    lm = WordNetLemmatizer()\n",
    "    features={}\n",
    "    voc_count = Counter()\n",
    "    text = text.lower()\n",
    "    text = re.sub('\"[\\S\\s]*?\"','',text)\n",
    "    lines = [len(x.split()) for x in sent_tokenize(text)]\n",
    "    avg_word_pl = sum(lines)/len(lines)    \n",
    "    l = [len(x.split()) for x in text.split('\\n')]\n",
    "    avg_word_pp = sum(l)/len(l)\n",
    "    features['avg_word_pp']=avg_word_pp\n",
    "    features['avg_word_pl']=avg_word_pl\n",
    "    toks = word_tokenize(text)\n",
    "    tok_tags = pos_tag(toks)\n",
    "#     print(toks)\n",
    "    lemmas=[]\n",
    "    words=[]\n",
    "    min_word_depths=Counter()\n",
    "    max_word_depths=Counter()\n",
    "    func_words = []\n",
    "    for word,tag in tok_tags:\n",
    "        words.append(word)\n",
    "        lemmas.append(lm.lemmatize(word,get_wordnet_pos(tag)))\n",
    "        wd_sysnset = wordnet.synsets(word,pos=get_wordnet_pos(tag))\n",
    "        if len(wd_sysnset)>0:\n",
    "            min_word_depths[wd_sysnset[0].min_depth()]+=1\n",
    "            max_word_depths[wd_sysnset[0].max_depth()]+=1\n",
    "        if(get_wordnet_pos(tag) =='a'):\n",
    "            func_words.append(word)\n",
    "            voc_count[word] +=1\n",
    "    unk_toks = set(lemmas)\n",
    "    unk_funk = set(func_words)\n",
    "    \n",
    "    type_tok_ratio = len(unk_toks)/len(toks)\n",
    "    funk_freq = len(unk_funk)/len(toks)\n",
    "    features['type_tok_ratio'] = type_tok_ratio\n",
    "#     features['type_tok_rat'] = len(set(words))/len(words)\n",
    "    features['funk_freq'] = funk_freq\n",
    "    for funk_word in voc_count:\n",
    "        features[funk_word+'_word_count'] = voc_count[funk_word]\n",
    "    for min_depth in min_word_depths:\n",
    "        features['min_word_depth'+str(min_depth)]=min_word_depths[min_depth]\n",
    "    for max_depth in max_word_depths:\n",
    "        features['min_word_depth'+str(max_depth)]=max_word_depths[max_depth]    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(path='./all_c'):\n",
    "    data=[]\n",
    "    c=0\n",
    "#     print(path)\n",
    "    for entry in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, entry)):\n",
    "            text = readtext(path+\"/\" +entry+'/'+entry+'.txt')\n",
    "            text = text.decode(\"utf-8\", \"replace\")\n",
    "            for article in text.split('\\t'):\n",
    "                if(len(article)>0):\n",
    "                    data.append((entry,featurize(article)))\n",
    "            auth_dict[c]=entry\n",
    "            auth_map[entry]=c\n",
    "            c+=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(path='./all_c'):\n",
    "    data=[]\n",
    "    for entry in os.listdir(path):\n",
    "#         print(entry)\n",
    "#         print(os.path.join(path, entry+'/test'))\n",
    "        if os.path.isdir(os.path.join(path, entry+'/test')):\n",
    "#             print(Path(path+'\\\\'+entry+'\\\\test'))\n",
    "            for tst_file in os.listdir(path+'/'+entry+'/test'):\n",
    "                if(tst_file.endswith('.txt')):\n",
    "                    text = readtext(path+'/'+entry+'/test/'+tst_file)\n",
    "                    text = text.decode(\"utf-8\", \"replace\")\n",
    "                    for article in text.split('\\t'):\n",
    "                        if(len(article)>0):\n",
    "                            data.append((entry,featurize(article)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(get_train_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_test_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    global all_feats\n",
    "    fid=0\n",
    "    data = get_train_data()\n",
    "    for (label,feats) in data:\n",
    "        for feat in feats:\n",
    "            if feat not in all_feats:\n",
    "                all_feats[feat] = fid\n",
    "                fid+=1\n",
    "    D = len(data)\n",
    "    F = len(all_feats)\n",
    "    X = sparse.dok_matrix((D,F))\n",
    "    Y = np.zeros(len(data))\n",
    "    for idx,(label,feats) in enumerate(data):\n",
    "        for feat in feats:\n",
    "            X[idx,all_feats[feat]] = feats[feat]\n",
    "            Y[idx] = auth_map[label]\n",
    "    logreg = linear_model.LogisticRegression(C=0.01)\n",
    "    logreg.fit(X,Y)\n",
    "#     print(logreg.score(X,Y))\n",
    "    return(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lg=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    test_data = get_test_data()\n",
    "    lm = create_model()\n",
    "    D = len(test_data)\n",
    "    F = len(all_feats)\n",
    "    X = sparse.dok_matrix((D,F))\n",
    "    Y = np.arange(len(test_data),dtype=int)\n",
    "    true_labels=[]\n",
    "    for idx,(label,feats) in enumerate(test_data):\n",
    "        true_labels.append(auth_map[label])\n",
    "        for feat in feats:\n",
    "            if feat in all_feats:\n",
    "                X[idx,all_feats[feat]] = feats[feat]\n",
    "    \n",
    "    preds=lm.predict(X)    \n",
    "    p,r,f1, _ = smt.precision_recall_fscore_support(true_labels,preds)\n",
    "    for i in range(len(auth_dict)):\n",
    "        print('Author',auth_dict[i])\n",
    "        print('precision',round(p[i],4))\n",
    "        print('recall',round(r[i],4))\n",
    "        print('f1score',round(f1[i],4))\n",
    "        print()\n",
    "    for i in range(len(true_labels)):\n",
    "        print('true author:',auth_dict[true_labels[i]])\n",
    "        print('predicted author:',auth_dict[preds[i]])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auth_dict)\n",
    "print(auth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

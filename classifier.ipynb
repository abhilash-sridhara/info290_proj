{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import sklearn.metrics as smt\n",
    "all_feats={}\n",
    "auth_dict={}\n",
    "auth_map={}\n",
    "l_c=0.01\n",
    "path='./all_c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtext(filepath):\n",
    "    text=''    \n",
    "    with open(filepath,'rb') as f:\n",
    "        text=f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(text):\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return 'a'\n",
    "    \n",
    "    lm = WordNetLemmatizer()\n",
    "    features={}\n",
    "    voc_count = Counter()\n",
    "    text = text.lower()\n",
    "    text = re.sub('\"[\\S\\s]*?\"','',text)\n",
    "    lines = [len(x.split()) for x in sent_tokenize(text)]\n",
    "    avg_word_pl = sum(lines)/len(lines)    \n",
    "    l = [len(x.split()) for x in text.split('\\n')]\n",
    "    avg_word_pp = sum(l)/len(l)\n",
    "    features['avg_word_pp']=avg_word_pp\n",
    "    features['avg_word_pl']=avg_word_pl\n",
    "    toks = word_tokenize(text)\n",
    "    tok_tags = pos_tag(toks)\n",
    "#     print(toks)\n",
    "    lemmas=[]\n",
    "    words=[]\n",
    "    min_word_depths=Counter()\n",
    "    max_word_depths=Counter()\n",
    "    func_words = []\n",
    "    for word,tag in tok_tags:\n",
    "        words.append(word)\n",
    "        lemmas.append(lm.lemmatize(word,get_wordnet_pos(tag)))\n",
    "        wd_sysnset = wordnet.synsets(word,pos=get_wordnet_pos(tag))\n",
    "        if len(wd_sysnset)>0:\n",
    "            min_word_depths[wd_sysnset[0].min_depth()]+=1\n",
    "            max_word_depths[wd_sysnset[0].max_depth()]+=1\n",
    "        if(get_wordnet_pos(tag) =='a'):\n",
    "            func_words.append(word)\n",
    "            voc_count[word] +=1\n",
    "    unk_toks = set(lemmas)\n",
    "    unk_funk = set(func_words)\n",
    "    \n",
    "    type_tok_ratio = len(unk_toks)/len(toks)\n",
    "    funk_freq = len(unk_funk)/len(toks)\n",
    "    features['type_tok_ratio'] = type_tok_ratio\n",
    "#     features['type_tok_rat'] = len(set(words))/len(words)\n",
    "    features['funk_freq'] = funk_freq\n",
    "    for funk_word in voc_count:\n",
    "        features[funk_word+'_word_count'] = voc_count[funk_word]\n",
    "    for min_depth in min_word_depths:\n",
    "        features['min_word_depth'+str(min_depth)]=min_word_depths[min_depth]\n",
    "    for max_depth in max_word_depths:\n",
    "        features['min_word_depth'+str(max_depth)]=max_word_depths[max_depth]    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    data=[]\n",
    "    c=0\n",
    "#     print(path)\n",
    "    for entry in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, entry)):\n",
    "            text = readtext(path+\"/\" +entry+'/'+entry+'.txt')\n",
    "            text = text.decode(\"utf-8\", \"replace\")\n",
    "            for article in text.split('\\t'):\n",
    "                if(len(article)>0):\n",
    "                    data.append((entry,featurize(article)))\n",
    "            auth_dict[c]=entry\n",
    "            auth_map[entry]=c\n",
    "            c+=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    data=[]\n",
    "    for entry in os.listdir(path):\n",
    "#         print(entry)\n",
    "#         print(os.path.join(path, entry+'/test'))\n",
    "        if os.path.isdir(os.path.join(path, entry+'/test')):\n",
    "#             print(Path(path+'\\\\'+entry+'\\\\test'))\n",
    "            for tst_file in os.listdir(path+'/'+entry+'/test'):\n",
    "                if(tst_file.endswith('.txt')):\n",
    "                    text = readtext(path+'/'+entry+'/test/'+tst_file)\n",
    "                    text = text.decode(\"utf-8\", \"replace\")\n",
    "                    for article in text.split('\\t'):\n",
    "                        if(len(article)>0):\n",
    "                            data.append((entry,featurize(article)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(get_train_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_test_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    global all_feats\n",
    "    fid=0\n",
    "    data = get_train_data()\n",
    "    for (label,feats) in data:\n",
    "        for feat in feats:\n",
    "            if feat not in all_feats:\n",
    "                all_feats[feat] = fid\n",
    "                fid+=1\n",
    "    D = len(data)\n",
    "    F = len(all_feats)\n",
    "    X = sparse.dok_matrix((D,F))\n",
    "    Y = np.zeros(len(data))\n",
    "    for idx,(label,feats) in enumerate(data):\n",
    "        for feat in feats:\n",
    "            X[idx,all_feats[feat]] = feats[feat]\n",
    "            Y[idx] = auth_map[label]\n",
    "    logreg = linear_model.LogisticRegression(C=0.01)\n",
    "    logreg.fit(X,Y)\n",
    "#     print(logreg.score(X,Y))\n",
    "    return(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lg=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    test_data = get_test_data()\n",
    "    lm = create_model()\n",
    "    D = len(test_data)\n",
    "    F = len(all_feats)\n",
    "    X = sparse.dok_matrix((D,F))\n",
    "    Y = np.arange(len(test_data),dtype=int)\n",
    "    true_labels=[]\n",
    "    for idx,(label,feats) in enumerate(test_data):\n",
    "        true_labels.append(auth_map[label])\n",
    "        for feat in feats:\n",
    "            if feat in all_feats:\n",
    "                X[idx,all_feats[feat]] = feats[feat]\n",
    "    \n",
    "    preds=lm.predict(X)\n",
    "    probs = lm.predict_proba(X)\n",
    "    idx=0\n",
    "    \n",
    "    p,r,f1, _ = smt.precision_recall_fscore_support(true_labels,preds,warn_for=())\n",
    "    preds = [int(x) for x in preds]\n",
    "    print(preds)\n",
    "    print(true_labels)\n",
    "    print()\n",
    "    for prob in probs:\n",
    "        print('true_label',true_labels[idx])\n",
    "        print(prob)\n",
    "        idx+=1\n",
    "    print()\n",
    "    w_c=0\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i]!=true_labels[i]:\n",
    "            w_c+=1\n",
    "    print(w_c,'incorrect out of',len(preds))\n",
    "    print()\n",
    "    for i in range(len(auth_dict)):\n",
    "        print('Author',auth_dict[i])\n",
    "        print('precision',round(p[i],4))\n",
    "        print('recall',round(r[i],4))\n",
    "        print('f1score',round(f1[i],4))\n",
    "        print()\n",
    "#     for i in range(len(true_labels)):\n",
    "#         print('true author:',auth_dict[true_labels[i]])\n",
    "#         print('predicted author:',auth_dict[preds[i]])\n",
    "#         print()\n",
    "    return({'avg f1':round(sum(f1)/len(f1),2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 0, 1, 7, 1, 2, 3, 2, 3, 3, 4, 4, 5, 5, 5, 10, 5, 7, 7, 5, 8, 7, 8, 9, 9, 2, 1]\n",
      "[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10]\n",
      "\n",
      "true_label 0\n",
      "[0.26611178 0.00453514 0.00407061 0.00386069 0.00957406 0.00493223\n",
      " 0.00486716 0.00590099 0.12359651 0.53163671 0.04091414]\n",
      "true_label 0\n",
      "[0.55212557 0.00716817 0.00674045 0.00907956 0.02163855 0.00100753\n",
      " 0.01368293 0.00508058 0.03879159 0.30832    0.03636507]\n",
      "true_label 1\n",
      "[0.00703536 0.38256152 0.02270463 0.08260056 0.22017464 0.04300634\n",
      " 0.09410314 0.00715273 0.01680035 0.08986367 0.03399707]\n",
      "true_label 1\n",
      "[0.0289821  0.29702401 0.05649812 0.00357808 0.01618085 0.02063079\n",
      " 0.00784509 0.35994868 0.17963563 0.01662414 0.0130525 ]\n",
      "true_label 1\n",
      "[2.34556568e-02 7.77742539e-01 8.94237267e-03 3.99852497e-03\n",
      " 1.55121969e-01 5.77720797e-03 6.71462503e-04 6.00580434e-03\n",
      " 6.73611284e-03 9.87572085e-03 1.67262989e-03]\n",
      "true_label 2\n",
      "[2.20802466e-03 2.26195351e-02 9.07474496e-01 9.27236322e-04\n",
      " 1.16794937e-03 5.57911707e-05 3.28951294e-02 7.97831640e-03\n",
      " 5.51877269e-03 8.04484432e-05 1.90743008e-02]\n",
      "true_label 2\n",
      "[0.00417902 0.00594078 0.05352036 0.28917816 0.01032126 0.24383204\n",
      " 0.04904977 0.022361   0.09592307 0.03707977 0.18861478]\n",
      "true_label 2\n",
      "[0.11066731 0.18145109 0.22263607 0.05115719 0.00610179 0.19568372\n",
      " 0.02750969 0.17697653 0.02139369 0.0017912  0.00463171]\n",
      "true_label 3\n",
      "[1.48869695e-02 7.65292107e-03 4.63963986e-04 8.78762108e-01\n",
      " 4.26763716e-03 1.37844561e-03 1.12518048e-02 1.79946064e-03\n",
      " 3.19649645e-02 4.28595509e-02 4.71217419e-03]\n",
      "true_label 3\n",
      "[0.03149521 0.02906241 0.01309336 0.77372545 0.01530486 0.00113726\n",
      " 0.01988724 0.00478084 0.04646831 0.0264754  0.03856966]\n",
      "true_label 4\n",
      "[1.37766537e-04 1.55581580e-01 3.56948740e-04 4.70500261e-03\n",
      " 7.80531051e-01 2.92653340e-02 4.01719147e-03 2.62062422e-03\n",
      " 4.96142873e-03 4.46179952e-03 1.33612730e-02]\n",
      "true_label 4\n",
      "[4.10810053e-03 1.77122254e-01 5.41817268e-04 4.42122067e-04\n",
      " 7.57042907e-01 6.06155957e-03 2.80086801e-03 4.28116220e-02\n",
      " 1.02893160e-03 2.33408134e-03 5.70573699e-03]\n",
      "true_label 5\n",
      "[0.01337516 0.12997965 0.01306077 0.00383698 0.03299844 0.75084512\n",
      " 0.00349534 0.00883701 0.006237   0.01107051 0.02626403]\n",
      "true_label 5\n",
      "[0.05336424 0.09058613 0.21212013 0.0179834  0.00891403 0.5928006\n",
      " 0.00131993 0.00576569 0.01318854 0.00196747 0.00198984]\n",
      "true_label 5\n",
      "[0.29772827 0.06900633 0.08246401 0.00532997 0.00062659 0.53096739\n",
      " 0.0007847  0.00625343 0.00181725 0.00355591 0.00146613]\n",
      "true_label 6\n",
      "[0.02691894 0.04972161 0.12366532 0.03617381 0.09458798 0.0350077\n",
      " 0.19346181 0.01135271 0.00411064 0.01463908 0.41036039]\n",
      "true_label 6\n",
      "[0.00778438 0.09908795 0.05993556 0.14570673 0.01115849 0.38794763\n",
      " 0.01710582 0.05688376 0.18586284 0.0188907  0.00963614]\n",
      "true_label 7\n",
      "[5.00784040e-02 1.33529523e-02 8.55078639e-03 1.36396805e-03\n",
      " 1.77537705e-02 4.32051273e-04 2.28385134e-03 7.41647579e-01\n",
      " 1.42536798e-01 1.64427508e-02 5.55708783e-03]\n",
      "true_label 7\n",
      "[0.13281863 0.23041699 0.02328752 0.03089928 0.00189008 0.13088391\n",
      " 0.00413315 0.30651869 0.05155794 0.08076653 0.00682728]\n",
      "true_label 7\n",
      "[0.10839491 0.03810795 0.04376883 0.00537873 0.01238804 0.33475315\n",
      " 0.08876758 0.18165672 0.07308513 0.084153   0.02954596]\n",
      "true_label 8\n",
      "[0.16747504 0.01131926 0.17911956 0.01481019 0.00838171 0.01415458\n",
      " 0.01967257 0.08990782 0.43042291 0.05431984 0.01041652]\n",
      "true_label 8\n",
      "[0.05853623 0.01117067 0.00871584 0.06517596 0.00919448 0.0008881\n",
      " 0.07239218 0.34914966 0.23350681 0.17420201 0.01706805]\n",
      "true_label 8\n",
      "[0.21811746 0.05812224 0.05350128 0.10630154 0.01388287 0.00629706\n",
      " 0.01652709 0.01635962 0.32358649 0.13776973 0.04953462]\n",
      "true_label 9\n",
      "[0.0555104  0.00981775 0.00145612 0.00150964 0.0024048  0.04590379\n",
      " 0.01462139 0.12320529 0.30804434 0.43567988 0.00184659]\n",
      "true_label 9\n",
      "[4.62523909e-02 1.99681319e-02 5.09533043e-04 1.49070449e-02\n",
      " 1.08473302e-01 5.98791097e-03 9.26342814e-03 3.67299840e-02\n",
      " 1.56661710e-01 5.97713366e-01 3.53319768e-03]\n",
      "true_label 10\n",
      "[0.15414913 0.20801288 0.23396932 0.01259971 0.061579   0.00396339\n",
      " 0.00501403 0.00064755 0.03274924 0.11866636 0.16864938]\n",
      "true_label 10\n",
      "[0.00290264 0.49379992 0.07610171 0.00093978 0.00234579 0.00088879\n",
      " 0.00575561 0.0183434  0.0022662  0.00095092 0.39570523]\n",
      "\n",
      "9 incorrect out of 27\n",
      "\n",
      "Author John Tierney\n",
      "precision 1.0\n",
      "recall 0.5\n",
      "f1score 0.6667\n",
      "\n",
      "Author Simon Romero\n",
      "precision 0.6667\n",
      "recall 0.6667\n",
      "f1score 0.6667\n",
      "\n",
      "Author Adam Liptak\n",
      "precision 0.6667\n",
      "recall 0.6667\n",
      "f1score 0.6667\n",
      "\n",
      "Author Alessandra Stanley\n",
      "precision 0.6667\n",
      "recall 1.0\n",
      "f1score 0.8\n",
      "\n",
      "Author Terry Pristin\n",
      "precision 1.0\n",
      "recall 1.0\n",
      "f1score 1.0\n",
      "\n",
      "Author Robert Pear\n",
      "precision 0.6\n",
      "recall 1.0\n",
      "f1score 0.75\n",
      "\n",
      "Author Sarah Kreshaw\n",
      "precision 0.0\n",
      "recall 0.0\n",
      "f1score 0.0\n",
      "\n",
      "Author Adam Nagoury\n",
      "precision 0.5\n",
      "recall 0.6667\n",
      "f1score 0.5714\n",
      "\n",
      "Author Maureen Dowd\n",
      "precision 1.0\n",
      "recall 0.6667\n",
      "f1score 0.8\n",
      "\n",
      "Author Nicholas\n",
      "precision 0.6667\n",
      "recall 1.0\n",
      "f1score 0.8\n",
      "\n",
      "Author Alan Ridding\n",
      "precision 0.0\n",
      "recall 0.0\n",
      "f1score 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhilash/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/abhilash/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg f1': 0.61}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auth_dict)\n",
    "print(auth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
